{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep"
      ],
      "metadata": {
        "id": "CbQJUdxobK2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rounakbanik/the-movies-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5et0a1jUngH",
        "outputId": "91620b09-6478-4a05-b3a8-ae995c0dd993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rounakbanik/the-movies-dataset?dataset_version_number=7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 228M/228M [00:11<00:00, 21.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rounakbanik/the-movies-dataset/versions/7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy scikit-surprise  # Remove all traces\n",
        "!pip install \"numpy==1.26.4\"            # Install last stable NumPy 1.x\n",
        "!pip install scikit-surprise --no-cache-dir  # Force rebuild without cached binaries\n",
        "\n",
        "# RESTART RUNTIME: Runtime → Restart runtime\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMy9W1gnaDF2",
        "outputId": "799a841e-9cde-4bd7-ab60-c0bbbd052a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[33mWARNING: Skipping scikit-surprise as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.26.4\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505211 sha256=a5533c4f9423557091884f9203e7437346f38592da54cc5714985d041226a06f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s3ea01h1/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, surprise\n",
        "print(f\"NumPy: {numpy.__version__}\")      # Should show 1.x.x\n",
        "print(f\"Surprise: {surprise.__version__}\")# Should show 1.1.x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBB-FRxzgQPJ",
        "outputId": "1180ca60-997b-45c5-9b18-948d39dd7507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 1.26.4\n",
            "Surprise: 1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/rounakbanik/the-movies-dataset/versions/7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L9nbYbbV4Zh",
        "outputId": "7caf6a79-493d-4f66-e404-84c63c8e6e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "credits.csv   links.csv        movies_metadata.csv  ratings_small.csv\n",
            "keywords.csv  links_small.csv  ratings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngebFOzNYimy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.accuracy import rmse, mae\n",
        "\n",
        "# Load ratings.csv\n",
        "ratings = pd.read_csv('/root/.cache/kagglehub/datasets/rounakbanik/the-movies-dataset/versions/7/ratings.csv')[['userId', 'movieId', 'rating']]\n",
        "\n",
        "# Define rating scale (e.g., 0.5-5.0)\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings, reader)\n",
        "\n",
        "# Split data into 80% train, 20% test\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Factorization"
      ],
      "metadata": {
        "id": "j6oh8yqwbPsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SVD model with 100 latent factors\n",
        "model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
        "model.fit(trainset)\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.test(testset)"
      ],
      "metadata": {
        "id": "QpUKb-4-YkWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate RMSE and MAE\n",
        "rmse_score = rmse(predictions)\n",
        "mae_score = mae(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdBGBL6JYqrh",
        "outputId": "3b05d3a3-7ea1-4059-d431-8c1c468bfd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.7966\n",
            "MAE:  0.6021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import cross_validate\n",
        "cross_validate(SVD(), data, measures=['rmse', 'mae'], cv=5, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "z1RWhubwYxJo",
        "outputId": "429d11a3-1a8b-4f90-c695-fa8f24ec36b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SVD' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-34087d92bb2d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'SVD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100, 150],\n",
        "    'lr_all': [0.002, 0.005, 0.01],\n",
        "    'reg_all': [0.02, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "gs.fit(data)\n",
        "\n",
        "print(f'Best RMSE: {gs.best_score[\"rmse\"]}')\n",
        "print(f'Optimal parameters: {gs.best_params[\"rmse\"]}')"
      ],
      "metadata": {
        "id": "OWowFaVrY0X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixture Model (Baseline + KNN + SVD)"
      ],
      "metadata": {
        "id": "oDz2b0kga8gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader, SVD, KNNBaseline, BaselineOnly\n",
        "from surprise.model_selection import cross_validate\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Concatenate, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Baseline estimates\n",
        "baseline = BaselineOnly()\n",
        "\n",
        "# Collaborative filtering (user-user)\n",
        "sim_options = {'name': 'pearson_baseline', 'user_based': True}\n",
        "knn = KNNBaseline(sim_options=sim_options)\n",
        "\n",
        "# Matrix factorization\n",
        "svd = SVD(n_factors=128, n_epochs=25, lr_all=0.005, reg_all=0.02)"
      ],
      "metadata": {
        "id": "KqROhcRda8Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network for blending\n",
        "def build_blender(n_users, n_items):\n",
        "    user_input = tf.keras.Input(shape=(1,))\n",
        "    item_input = tf.keras.Input(shape=(1,))\n",
        "\n",
        "    user_embed = Embedding(n_users, 64)(user_input)\n",
        "    item_embed = Embedding(n_items, 64)(item_input)\n",
        "\n",
        "    merged = Concatenate()([Flatten()(user_embed), Flatten()(item_embed)])\n",
        "    dense = Dense(128, activation='relu')(merged)\n",
        "    output = Dense(1)(dense)\n",
        "\n",
        "    return tf.keras.Model(inputs=[user_input, item_input], outputs=output)"
      ],
      "metadata": {
        "id": "MY9cVZzzbYfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridModel:\n",
        "    def __init__(self, models, blender):\n",
        "        self.models = models\n",
        "        self.blender = blender\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        # Train base models\n",
        "        for model in self.models:\n",
        "            model.fit(trainset)\n",
        "\n",
        "        # Generate blended features\n",
        "        user_ids = [uid for (uid, _, _) in trainset.all_ratings()]\n",
        "        item_ids = [iid for (_, iid, _) in trainset.all_ratings()]\n",
        "        predictions = np.array([\n",
        "            [model.predict(uid, iid).est for model in self.models]\n",
        "            for uid, iid in zip(user_ids, item_ids)\n",
        "        ])\n",
        "\n",
        "        # Train blender\n",
        "        self.blender.fit(\n",
        "            x=[np.array(user_ids), np.array(item_ids)],\n",
        "            y=np.array([r for (_, _, r) in trainset.all_ratings()]),\n",
        "            epochs=10,\n",
        "            batch_size=1024\n",
        "        )\n",
        "\n",
        "    def predict(self, uid, iid):\n",
        "        base_preds = [model.predict(uid, iid).est for model in self.models]\n",
        "        nn_pred = self.blender.predict([\n",
        "            np.array([uid]),\n",
        "            np.array([iid])\n",
        "        ])[0][0]\n",
        "        return np.mean([*base_preds, nn_pred])"
      ],
      "metadata": {
        "id": "aVaX8-Qebg3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize components\n",
        "n_users = ratings.userId.nunique()\n",
        "n_items = ratings.movieId.nunique()\n",
        "blender = build_blender(n_users, n_items)\n",
        "hybrid = HybridModel([baseline, knn, svd], blender)\n",
        "\n",
        "# Cross-validate\n",
        "cross_validate(hybrid, data, measures=['rmse', 'mae'], cv=3, verbose=True)"
      ],
      "metadata": {
        "id": "XQM994jEbjnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "# Tune SVD component\n",
        "svd_params = {\n",
        "    'n_factors': [64, 128],\n",
        "    'lr_all': [0.003, 0.005],\n",
        "    'reg_all': [0.02, 0.04]\n",
        "}\n",
        "gs_svd = GridSearchCV(SVD, svd_params, measures=['rmse'], cv=3)\n",
        "gs_svd.fit(data)\n",
        "\n",
        "# Tune KNN component\n",
        "knn_params = {'k': [20, 40], 'sim_options': {'name': ['pearson_baseline']}}\n",
        "gs_knn = GridSearchCV(KNNBaseline, knn_params, measures=['rmse'], cv=3)\n",
        "gs_knn.fit(data)"
      ],
      "metadata": {
        "id": "Trw5GEHpbmM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_svd = gs_svd.best_estimator['rmse']\n",
        "best_knn = gs_knn.best_estimator['rmse']\n",
        "optimized_hybrid = HybridModel([baseline, best_knn, best_svd], blender)"
      ],
      "metadata": {
        "id": "TBjRuNNBboEd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}